<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Auditing large language models for race & gender disparities: Implications for artificial intelligence–based hiring
</title>
    <link rel="stylesheet" href="/assets/css/styles.css?v=3">

    <!-- Add MathJax Latex Support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Add Custom Javascript -->
    

    <link type="application/atom+xml" rel="alternate" href="https://www.jgaeb.com/feed.xml" title="Jgaeb" />

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Auditing large language models for race &amp; gender disparities: Implications for artificial intelligence–based hiring | Jgaeb</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Auditing large language models for race &amp; gender disparities: Implications for artificial intelligence–based hiring" />
<meta name="author" content="Johann D. Gaebler" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Rapid advances in artificial intelligence (AI), including large language models (LLMs) with abilities that rival those of human experts on a wide array of tasks, are reshaping how people make important decisions. At the same time, critics worry that LLMs may inadvertently discriminate against some groups. To address these concerns, recent regulations call for auditing the LLMs used in important decisions such as hiring. But neither current regulations nor the scientific literature offers clear guidance on how to conduct these audits. In this article, we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgments. We applied this method to a range of LLMs instructed to rate job candidates using a novel data set of job applications for K-12 teaching positions in a large American public school district. By altering the application materials to imply that candidates are members of specific demographic groups, we measured the extent to which race and gender influenced the LLMs’ ratings of the candidates’ suitability. We found moderate race and gender disparities, with the models slightly favoring women and non-White candidates. This pattern persisted across several variations in our experiment. It is unclear what might be driving these disparities, but we hypothesize that they stem from posttraining efforts, which are part of the LLM training process and intended to correct biases in these models. We conclude by discussing the limitations of correspondence experiments for auditing algorithms." />
<meta property="og:description" content="Rapid advances in artificial intelligence (AI), including large language models (LLMs) with abilities that rival those of human experts on a wide array of tasks, are reshaping how people make important decisions. At the same time, critics worry that LLMs may inadvertently discriminate against some groups. To address these concerns, recent regulations call for auditing the LLMs used in important decisions such as hiring. But neither current regulations nor the scientific literature offers clear guidance on how to conduct these audits. In this article, we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgments. We applied this method to a range of LLMs instructed to rate job candidates using a novel data set of job applications for K-12 teaching positions in a large American public school district. By altering the application materials to imply that candidates are members of specific demographic groups, we measured the extent to which race and gender influenced the LLMs’ ratings of the candidates’ suitability. We found moderate race and gender disparities, with the models slightly favoring women and non-White candidates. This pattern persisted across several variations in our experiment. It is unclear what might be driving these disparities, but we hypothesize that they stem from posttraining efforts, which are part of the LLM training process and intended to correct biases in these models. We conclude by discussing the limitations of correspondence experiments for auditing algorithms." />
<link rel="canonical" href="https://www.jgaeb.com/papers/audit.html" />
<meta property="og:url" content="https://www.jgaeb.com/papers/audit.html" />
<meta property="og:site_name" content="Jgaeb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-19T22:11:34+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Auditing large language models for race &amp; gender disparities: Implications for artificial intelligence–based hiring" />
<meta name="twitter:site" content="@jgaeb1" />
<meta name="twitter:creator" content="@Johann D. Gaebler" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Johann D. Gaebler"},"dateModified":"2025-08-19T22:11:34+00:00","datePublished":"2025-08-19T22:11:34+00:00","description":"Rapid advances in artificial intelligence (AI), including large language models (LLMs) with abilities that rival those of human experts on a wide array of tasks, are reshaping how people make important decisions. At the same time, critics worry that LLMs may inadvertently discriminate against some groups. To address these concerns, recent regulations call for auditing the LLMs used in important decisions such as hiring. But neither current regulations nor the scientific literature offers clear guidance on how to conduct these audits. In this article, we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgments. We applied this method to a range of LLMs instructed to rate job candidates using a novel data set of job applications for K-12 teaching positions in a large American public school district. By altering the application materials to imply that candidates are members of specific demographic groups, we measured the extent to which race and gender influenced the LLMs’ ratings of the candidates’ suitability. We found moderate race and gender disparities, with the models slightly favoring women and non-White candidates. This pattern persisted across several variations in our experiment. It is unclear what might be driving these disparities, but we hypothesize that they stem from posttraining efforts, which are part of the LLM training process and intended to correct biases in these models. We conclude by discussing the limitations of correspondence experiments for auditing algorithms.","headline":"Auditing large language models for race &amp; gender disparities: Implications for artificial intelligence–based hiring","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.jgaeb.com/papers/audit.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.jgaeb.com/assets/img/headshot.png"},"name":"Johann D. Gaebler"},"url":"https://www.jgaeb.com/papers/audit.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header>
      <h1>Johann D. Gaebler</h1>
      <p class="contact-info"><a href="mailto:me@jgaeb.com">Email</a> • <a href="https://github.com/jgaeb">GitHub</a> • <a href="https://twitter.com/jgaeb1">Twitter</a></contact>
    </header>
    <nav>
      <div class="wrapper-nav-items">

  <div class="nav-item-container">
    <a href="/" >
      Home
    </a>
  </div>

  <div class="nav-item-container">
    <a href="/about.html" >
      About
    </a>
  </div>

  <div class="nav-item-container">
    <a href="/research.html" >
      Research
    </a>
  </div>

  <div class="nav-item-container">
    <a href="/blog.html" >
      Blog
    </a>
  </div>

</div>

    </nav>
    <main>
      <article class="paper">
  <section>
    <h1>Auditing large language models for race & gender disparities: Implications for artificial intelligence–based hiring
</h1>
    <p class="authors">Johann D. Gaebler, Sharad Goel, Aziz Huq, and Prasanna Tambe</p>
    <p class="biblio">
      <em>Behavioral Science & Policy</em>,
      2025.
    </p>

    

    
    <p class="biblio">
      ArXiv: <a href="https://arxiv.org/abs/2404.03086">2404.03086</a>.
    </p>
    

    
    <p class="biblio">
      DOI: <a href="https://doi.org/10.1177/23794607251320229">10.1177/23794607251320229</a>.
    </p>
    

    

    <h2>Abstract</h2>
    <p>Rapid advances in artificial intelligence (AI), including large language models
(LLMs) with abilities that rival those of human experts on a wide array of
tasks, are reshaping how people make important decisions. At the same time,
critics worry that LLMs may inadvertently discriminate against some groups. To
address these concerns, recent regulations call for auditing the LLMs used in
important decisions such as hiring. But neither current regulations nor the
scientific literature offers clear guidance on how to conduct these audits. In
this article, we propose and investigate one approach for auditing algorithms:
<em>correspondence experiments</em>, a widely applied tool for detecting bias in human
judgments. We applied this method to a range of LLMs instructed to rate job
candidates using a novel data set of job applications for K-12 teaching
positions in a large American public school district. By altering the
application materials to imply that candidates are members of specific
demographic groups, we measured the extent to which race and gender influenced
the LLMs’ ratings of the candidates’ suitability. We found moderate race and
gender disparities, with the models slightly favoring women and non-White
candidates. This pattern persisted across several variations in our experiment.
It is unclear what might be driving these disparities, but we hypothesize that
they stem from posttraining efforts, which are part of the LLM training process
and intended to correct biases in these models. We conclude by discussing the
limitations of correspondence experiments for auditing algorithms.</p>

  </section>

  <section>
    

    
  </section>
</article>

    </main>
    <footer>
  
  <p>
    &copy; 2021–2025 Johann D. Gaebler.
    Last updated August 19, 2025.
  </p>
</footer>

  </body>
</html>
